/**
 * Free Model Provider
 * Simple provider for testing with free models
 */

const logger = require('../../utils/logger');

class FreeModelProvider {
  constructor() {
    this.name = 'Free Model Provider';
    this.type = 'free-local';
    this.models = {
      'llama3.1-8b': {
        name: 'Llama 3.1 8B',
        provider: 'Meta',
        description: 'Open source and customizable',
        temperature: 0.7,
        maxTokens: 2048,
        category: 'general'
      },
      'mistral-7b': {
        name: 'Mistral 7B',
        provider: 'Mistral AI',
        description: 'Efficient European model',
        temperature: 0.7,
        maxTokens: 2048,
        category: 'general'
      },
      'phi3-3.8b': {
        name: 'Phi-3 Mini',
        provider: 'Microsoft',
        description: 'Compact but powerful',
        temperature: 0.7,
        maxTokens: 2048,
        category: 'general'
      },
      'codellama-7b': {
        name: 'CodeLlama',
        provider: 'Meta',
        description: 'Specialized for coding tasks',
        temperature: 0.7,
        maxTokens: 2048,
        category: 'coding'
      }
    };
  }

  // Check if model is supported
  async supportsModel(modelId) {
    return this.models[modelId] !== undefined;
  }

  // Get model info
  getModelInfo(modelId) {
    return this.models[modelId] || null;
  }

  // Generate text response
  async generateText(modelId, prompt, options = {}, userId = null, userTier = 'free') {
    try {
      const model = this.models[modelId];
      if (!model) {
        throw new Error(`Model ${modelId} not supported`);
      }

      // Simulate response time
      const responseTime = Math.random() * 2000 + 500; // 500-2500ms
      await new Promise(resolve => setTimeout(resolve, responseTime));

      // Generate simple response based on model and language
      let response = '';
      const isChinese = options.language === 'zh-CN' || /[\u4e00-\u9fff]/.test(prompt);
      
      if (modelId === 'codellama-7b') {
        if (isChinese) {
          response = `作为 CodeLlama，我可以帮助您处理编程任务。这是对"${prompt}"的回复：\n\n这是来自 ${model.name} 模型的模拟回复。在实际实现中，这将是一个真正的代码生成或分析回复。`;
        } else {
          response = `As CodeLlama, I can help you with coding tasks. Here's a response to: "${prompt}"\n\nThis is a simulated response from the ${model.name} model. In a real implementation, this would be an actual code generation or analysis response.`;
        }
      } else if (modelId === 'mistral-7b') {
        if (isChinese) {
          response = `作为 Mistral 7B，我可以帮助您处理各种任务。这是对"${prompt}"的回复：\n\n这是来自 ${model.name} 模型的模拟回复。Mistral 以其效率和欧洲训练数据而闻名。`;
        } else {
          response = `As Mistral 7B, I can help you with various tasks. Here's my response to: "${prompt}"\n\nThis is a simulated response from the ${model.name} model. Mistral is known for its efficiency and European training data.`;
        }
      } else if (modelId === 'phi3-3.8b') {
        if (isChinese) {
          response = `作为 Phi-3 Mini，我可以协助您处理请求。这是对"${prompt}"的回复：\n\n这是来自 ${model.name} 模型的模拟回复。Phi-3 设计为紧凑而强大，适用于各种任务。`;
        } else {
          response = `As Phi-3 Mini, I can assist you with your request. Here's my response to: "${prompt}"\n\nThis is a simulated response from the ${model.name} model. Phi-3 is designed to be compact yet powerful for various tasks.`;
        }
      } else {
        if (isChinese) {
          response = `作为 Llama 3.1 8B，我可以帮助您回答问题。这是对"${prompt}"的回复：\n\n这是来自 ${model.name} 模型的模拟回复。Llama 3.1 是一个开源模型，可以处理广泛的任务。`;
        } else {
          response = `As Llama 3.1 8B, I can help you with your question. Here's my response to: "${prompt}"\n\nThis is a simulated response from the ${model.name} model. Llama 3.1 is an open-source model that can handle a wide range of tasks.`;
        }
      }

      // Add some context about the user's tier
      if (isChinese) {
        response += `\n\n[注意：您正在使用 ${userTier} 层级。此回复由免费模型提供商生成。]`;
      } else {
        response += `\n\n[Note: You're using the ${userTier} tier. This response was generated by the free model provider.]`;
      }

      logger.info(`Free model response generated for user ${userId}, model: ${modelId}`);

      return {
        success: true,
        text: response,
        responseTime: Math.round(responseTime),
        tokens: Math.floor(response.length / 4), // Rough token estimation
        model: model.name,
        provider: this.name
      };

    } catch (error) {
      logger.error('Free model generation error:', error);
      return {
        success: false,
        error: error.message,
        text: 'Sorry, I encountered an error while processing your request. Please try again.',
        responseTime: 0,
        tokens: 0
      };
    }
  }

  // Generate streaming response
  async generateTextStream(modelId, prompt, options = {}, userId = null, userTier = 'free') {
    try {
      const model = this.models[modelId];
      if (!model) {
        throw new Error(`Model ${modelId} not supported`);
      }

      // Create a readable stream
      const { Readable } = require('stream');
      const stream = new Readable({
        read() {}
      });

      // Generate response in chunks
      const response = await this.generateText(modelId, prompt, options, userId, userTier);
      
      if (!response.success) {
        stream.push(JSON.stringify({ error: response.error }));
        stream.push(null);
        return stream;
      }

      // Split response into chunks and stream them
      const chunks = response.text.split(' ');
      let index = 0;

      const sendChunk = () => {
        if (index < chunks.length) {
          const chunk = chunks[index] + (index < chunks.length - 1 ? ' ' : '');
          stream.push(chunk);
          index++;
          setTimeout(sendChunk, 50); // 50ms per word (0.05 seconds)
        } else {
          stream.push(null);
        }
      };

      // Start streaming with moderate delay
      setTimeout(sendChunk, 150);

      return stream;

    } catch (error) {
      logger.error('Free model stream error:', error);
      const { Readable } = require('stream');
      const stream = new Readable({
        read() {}
      });
      stream.push(JSON.stringify({ error: error.message }));
      stream.push(null);
      return stream;
    }
  }

  // Health check
  async healthCheck() {
    return {
      healthy: true,
      provider: this.name,
      models: Object.keys(this.models),
      timestamp: new Date().toISOString()
    };
  }

  // Get available models
  async getAvailableModels() {
    return Object.keys(this.models).map(modelId => ({
      id: modelId,
      ...this.models[modelId]
    }));
  }
}

module.exports = FreeModelProvider; 